**NOTE: This folder is redundant and will receive no consistent further updates. The directory was migrated to (https://github.com/DAndrewA/a-guide-to-optimised-spatiotemporal-data-co-location-by-mutual-information-maximisation)** 


# Example: ATL09 and Cloudnet data

The code in this repository will demonstrate the use of the satellite validation pipeline code, as well as the framework outlined in "A guide to optimised spatiotemporal data co-location by mutual information maximisation".

## Shell environment setup

Because the framework deals with large data volumes, two different environment variables need to be set, to allow the scripts to locate raw data and save results to the appropriate locations.

+ The `$MI_MAXIMISATION_RAW_DATA_DIRECTORY` should point to the root directory where the raw ATL09 and Cloudnet data will be stored.
+ The `$MI_MAXIMISATION_RESULTS_DIRECTORY` should point to the root directpry where results and intermediate files will be stored.

We recommend that these are set through exporting the variables in one of the runtime configuration `.*rc` scripts in your user's home directory.

## Python environement setup

This work was developed with a mamba (conda) environment, named `overpass_analysis_again`.
This environment can be reproduced through the commands:

```bash
conda create --file requirements.yml [OPTIONS]
conda activate overpass_analysis_again
pip install -e ../
```

The pip installation command ensures that `sat_val_framework` is an editable install in the environment and can be imported.

## Directory structure

Having setup the environment variables above, they need to be instantiated correctly for the scripts to be able to locate data and run effectively.
The `$MI_MAXIMISATION_RAW_DATA_DIRECTORY` requires the following substructure in order to function properly.

```
$MI_MAXIMISATION_RAW_DATA_DIRECTORY/
├─ sites/
│  ├─ hyytiala/
│  │  ├─ atl09
│  │  ├─ cloudnet
│  ├─ juelich/
│  │  ├─ atl09
│  │  ├─ cloudnet
│  ├─ munich/
│  │  ├─ atl09
│  │  ├─ cloudnet
│  ├─ ny-alesund/
│  │  ├─ atl09/
│  │  ├─ cloudnet/
```
Raw ATL09 `.h5` files are placed in the corresponding `atl09` subdirectories of each site, and Cloudnet `categorize.nc` files are placed in the `cloudnet` subdirectories of the corresponding sites.

## Downloading data
### ATL09

The ATL09 data can be downloaded as granules directly from the NSIDC (https://doi.org/10.5067/ATLAS/ATL09.006), or through the NASA Harmony API as described in the `downloading_atl09` directory.
We recommend the NASA Harmony API approach, as spatiotemporal subsetting is applied to the data prior to download, significantly reducing local storage requirements.
Storage requirements for uncompressed ATL09 data for Ny-Alesund, Hyytiala, Julich and Munich amounts to `725G + 287G + 208G + 198G = 1.38T`.

### Cloudnet

The Cloudnet categorize data product can be downloaded from the cloudnet FMI website ()
The specific collection is all `categorize` data for the sites with ids `ny-alesund`, `hyytiala`, `juelich` and `munich`, for the date range (YYYY-MM-DD) 2018-10-01 to 2025-01-01 inclusive.
The data used in this study are generated by the Aerosol, Clouds and Trace Gases Research Infrastructure (ACTRIS) and are available from the ACTRIS Data Centre using the following link: https://doi.org/10.60656/726097978e364d06.
Storage requirements for the Cloudnet data across all sites is `86.6G`.

### Results

Pre-generated results files can be downloaded from (10.5281/zenodo.17817304).
The repository contains two zip files, one associated with each of the environment variables named above.
The files can be unzipped in the appropriate locations, and the analysis to produce the figures and table results should be runnable.
Storage requirements for the uncompressed results files are `3.6G + 168M = 3.76G`

## Running scripts

Within `atl09_cloudnet/scripts` are the following scripts:

1. `get_collocation_events` produces pickled lists of co-location events describing what raw data needs loading for analysis.
2. `consolidate_collocation_event_lists` merges the batch outputs from `get_collocation_events` for a given site, if batching was used.
3. `compute_vcfs_per_event` produces a netcdf file with paired ATL09 and Cloudnet VCF profiles for all of the co-location events for which there is sufficient data given the provided co-location parametrisation.
4. `consolidate_vcfs_per_event_batch` consolidates `vcf_per_event` output files if the batching option was selected.
5. `compute_MI_with_confidence` computes datasets containing mutual information between the ATL09 and Cloudnet VCF profiles for a given co-location parametrisation.
6. `merge_MI_with_confidence` merges the outputs from `compute_MI_with_confidence`, producing `MI_merged.nc` which forms the basis for the analysis.

In order to run the scripts, ensure that the correct conda environment is loaded, and then run the scripts from this directory via the commands
```bash
python -m atl09_cloudnet.scripts.<script name> [OPTIONS]
```
Each script should provide suitable help messages for the option flags.

The results were computed on the JASMIN supercomputer ( https://www.jasmin.ac.uk ; DOI: 10.1109/BigData.2013.6691556 ) and as a result, SLURM submission generation scripts exist under `atl09_cloudnet/slurm`.
Write the output of the `echo-submission-script` scripts to a file which can then be called.
NOTE: the `--qos` and `--account` fields will need changing for the job submission to be permitted.

## Analysing results

Between the outputs produced by running the above scripts, and the raw data stored within the directory structure under `$MI_MAXIMISATION_RAW_DATA_DIRECTORY`, this should be sufficient for running the subsequent analysis to produce the analysis used in the paper.
The scripts for running the analysis and producing the resulting figures are found within the `figures` directory.
Each figure has its own script implementing the analysis, with common functionality shared within `figures/common`.
Paper-like figures with your own results should be possible to produce by using the inkscape-generated `figures/**/f??.svg` files and rendering these to .pdf files.
The analysis scripts should be ran within the directory they are found in.

